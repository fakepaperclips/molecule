If there's one aspect of the design that stands out, users will latch onto that. 
Ex: movement for Alterwear displays.

Users really need to see final working prototypes, in-situ, w/ as much specific details as possible to fully imagine.

Just asking users to brainstorm for you feels....shitty.
I need to read more about running good/fruitful user studies.

Another way to reward participants...can we send them the paper once it's published? Does that violate IRB? What if we invite people to "sign up to be notified when a paper is published" and then they're not necessarily users, they're interested parties. Could also be useful for publicity. Basically a hyper-specific citation notification service.

More follow-ups in recruiting surveys:
	- Asked if they used trackers, some specified no, could have asked a follow up here (does that bias it too much?)

Cesar's user study feedback
	- Think about why you're choosing this quote.
	- Someone reading this paper should think "oh that's exactly my user, let me see what issues they ran into". Stu is someone who has a lot of familiarity w/ the X/Y process, what is similar/where do they diverge? Something like "I can do things with this machine I couldn't do before" can be summarized in the quantitative Likert scales. Or, if you don't have enough participants to report numbers, say it in the qualitative section: 5/6 of the participants said they could do things they could do before.
	- What are the things that other people can take away, if they want to build devices like these. "Oh I really need it to be visual" "I love how tactile it is"
	- You should always do a Likert scale. The only way you can get feedback without you introducing social desireability bias. Make sure you let them fill out this part without you watching/hovering/knowing their answers.
	- "I always do the figures first. If I can explain the idea in a figure, the rest will follow."


AlterWear 2.0:
	- Specific Likert scale re:
		- Battery use
		- Whether tagging gesture feels intimate
		- 

Contextual fashion
	- Safety lighting on clothes revealed only at night
	- vibration on shoulder when you fall asleep in class
	- boundary-crossing interfaces
		- day to night (fashion-wise)
		- safety lighting on clothing revealed only at night

Color, Texture, and Silhouette


1. Tesselation origami w/ fabric 
	--> Michael will send Molly resources, Molly
	--> Arianna 
	--> Make a collaborative pinterst board (Molly)
	- interfacing
	- magnets
	- SMA
2. Weaving SMA into fabric/patterns - Michael
3. Actuate in the Z direction
	- paper cut-outs, but w/ fabric
	- power w/ magnetic fields: feel resistance, no 

Magnets + robots
	- build magnetic vocabulary
	- build robot vocabulary
--> Arianna wants to play around w/ thermochromic pigment.

Create shadows, detecting where light-sources are coming from
	- 

#### Performance-inspired HCI research

- Informed telepresence: 
	- A little breath sensor, and a breathing interface on the other end, to facilitate long-distance communication.
	- would be nice if you didn't need a specific breath sensor, if you could get it from a microphone or webcam somehow.
	- Breathing interface needs to somehow display: sighs, normal breathing, holding breath, deep breath, laugh cough.

- Meta-analysis of research projects that release any code or documentation with their papers
	- No analysis of code/documentation quality, or whether or not it still works. 
	- Could do that too: 

- A kindle-holder that has a bunch of pages
	- System that allows the correct number of pages
	- Cover and binding that changes (randomly?) to show book-specific imagery
	- How about an entire e-ink bookshelf that displays all titles, and you can "grab" one that gets loaded into the kindle.
	- Pages somehow connected that trigger a kindle page-turn when they are turned. 
	- Two kindles? So you get both sides?

- System (ML? Something?) that automatically finds the "key" gestures
	- Can you automatically identify the movement qualities that convey something? E.g., how do you really communicate a "duck"? Could you have a system that could take an arbitrary cloth and animate it? Or use "hands" to create shapes, and could identify the right way to compose a new character?
	- Maybe talk w/ Anca about this?
	- A generative system like the Northwestern music project that makes suggestions and hones those suggestions based on user input. 










