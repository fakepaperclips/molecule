#### Agenda

- Regular meeting time
- AR stuff update
  - Student: Founder and instructor of AR/VR decal - wants to stick around for 5th year Masters. Has access to lots of hardware. Knows high-level about the project.
  - Eric will put us in touch. Make a time to meet w/ Student. 
  - Hardware side: magnets? ("you did more than you think you did"). Something that's physical. Could be coupled w/ AR. Back to gestural textiles. How do you do some kinetics of something moving (physically, programming). 
  - Appearing interface. Me: "already been done." Reframing by Eric: "pneumatic work, things w/ heat, magnetics, etc." Don't halt all operations if I find something. If I find related work, ping Eric w/ it and he'll help me navigate it. Even Inform exists. Variation on soft materials w/ clothing. What is a button or knob on clothing. Cindy Kao has already done stuff but "they haven't done all things." Doesn't know latest. Thematically: all robot driving around. Stuff w/ eInk. Robot drives around is too close. Small little button interactions. Some ferrofluid thing on a surface...some magnet thing....(what's his criteria here? no insight into his method). No one's dealt with what happens in crowds - vocabulary w/ interactions. Wrinkled -> unwrinkled. Things you can change w/ UV. Me: "I need the big story / vision first." Individual empowered to be more of a personally designed things. Cosmetic Computing stuff. Accessibility (impaired vision...insta-braille). Citizen Science is public/citizen-democracy story. 
- Passive On-Body Wearable update. 
  - We have access to Anna's high-end printer.
  - Scaled up AlterWear. 
  - How well does the localization work? How would you author a stage-play perf w this?
- IRB protocol for Inchworm: Evaluating Tactile Feedback of Micro-Mechanical Systems. 2018-02-10760
- Update board.

**Lower priority**
- Undergrad 2-second check-in
- [Petition to count CS294-133 "Collaborative Intelligent Agents" as a Systems breadth](https://drive.google.com/file/d/1VgmwqgFxMfP3Qa-Yd5p0-WZMb8C2ygms/view)

- AR stuff update
  - Ideas
    - Video as input: What about "movement" as input? Could you extract "something" from a video of something moving? Like style transfer for videos I guess? Or like Kimiko's ["I/O Brush"](http://tangible.media.mit.edu/project/io-brush/) but for AR. Sampling real world for clothing.
    - Body movement as input: Kinesthetic learning, but for actuated AR systems (broad definition of actuated); on-body sensors; "arbitrary" mapping from body movement to whatever actuation - individual for each actuator.
    - Gesture (to camera) as Input. (Current graphics challenge).
    - Voodoo Doll (mini-version, or manipulating external device).
  - Related work
    - Vision-based hand-tracking.
      - Best: Gloves.
      - Very challenging problem for CV: 1) self-occlusions, 2) processing speed (but in 2007...different now?), 3) 
        - Erol, Ali, et al. ["Vision-based hand pose estimation: A review."](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.76.6351&rep=rep1&type=pdf) Computer Vision and Image Understanding 108.1-2 (2007): 52-73.
    - Focus on input w/ custom set of vibration sensors worn in armband
      - Chris Harrison, Desney Tan, and Dan Morris. 2010. [Skinput: appropriating the body as an input surface.](https://dl.acm.org/citation.cfm?id=1753394) In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (CHI '10). ACM, New York, NY, USA, 453-462. DOI: https://doi.org/10.1145/1753326.1753394
    - More input, depth-sensor this time.
      - Chris Harrison, Hrvoje Benko, and Andrew D. Wilson. 2011. [OmniTouch: wearable multitouch interaction everywhere.](https://dl.acm.org/citation.cfm?id=2047255) In Proceedings of the 24th annual ACM symposium on User interface software and technology (UIST '11). ACM, New York, NY, USA, 441-450. DOI: https://doi.org/10.1145/2047196.2047255
    - [Code repos w/ citations](https://github.com/xinghaochen/awesome-hand-pose-estimation). And another [link](https://github.com/xinghaochen/awesome-hand-pose-estimation). [Implementation](https://github.com/lzddzh/HandPoseEstimation) w/ single-depth image from Chi Xu's paper.  [Implementation](https://github.com/hjurong/hand-pose-estimation) from Qian et al.,'s paper.
    - Another paper on "existing challenges", but from 2007 again. [Link.] (https://www.researchgate.net/publication/3421786_Gesture_Recognition_A_Survey) 
    
